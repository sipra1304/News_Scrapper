{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCEnBo6275Ww"
      },
      "outputs": [],
      "source": [
        "# Suppress only the single InsecureRequestWarning\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "# Path to save output (modify as per your environment)\n",
        "OUTPUT_PATH = \"./news_archive_data/\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "def compute_starttime(date_obj, reference_date=date(2013, 6, 1), reference_value=41426):\n",
        "    \"\"\"\n",
        "    Compute an offset-based starttime used in some archive URLs.\n",
        "\n",
        "    Args:\n",
        "        date_obj (datetime.date): The target date.\n",
        "        reference_date (datetime.date): A base date for offset calculation.\n",
        "        reference_value (int): Starttime value corresponding to the reference_date.\n",
        "\n",
        "    Returns:\n",
        "        int: Offset-based date parameter.\n",
        "    \"\"\"\n",
        "    return reference_value + (date_obj - reference_date).days\n",
        "\n",
        "def build_archive_url(base_url_template, year, month, day):\n",
        "    \"\"\"\n",
        "    Construct the archive URL based on a date and template.\n",
        "\n",
        "    Args:\n",
        "        base_url_template (str): A URL template with placeholders {year}, {month}, {day}, {starttime}\n",
        "        year, month, day (int): Target date\n",
        "\n",
        "    Returns:\n",
        "        str: Constructed archive URL\n",
        "    \"\"\"\n",
        "    date_obj = date(year, month, day)\n",
        "    starttime = compute_starttime(date_obj)  # Optional usage if the site needs it\n",
        "    return base_url_template.format(year=year, month=month, day=day, starttime=starttime)\n",
        "\n",
        "def parse_article_links(html_content, base_url, filter_func):\n",
        "    \"\"\"\n",
        "    Parses HTML and extracts relevant article info using a custom filter function.\n",
        "\n",
        "    Args:\n",
        "        html_content (str): HTML page content\n",
        "        base_url (str): Base URL for building full article links\n",
        "        filter_func (function): A function to identify valid article URLs from <a> tags\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: List of articles with metadata\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    links = soup.find_all('a', href=True)\n",
        "\n",
        "    articles = []\n",
        "    for link in links:\n",
        "        href = link['href']\n",
        "        if filter_func(href):\n",
        "            category = href.strip(\"/\").split(\"/\")[0]  # Customize based on site's URL pattern\n",
        "            articles.append({\n",
        "                \"headline_text\": link.get_text(strip=True),\n",
        "                \"headline_category\": category,\n",
        "                \"article_link\": href if href.startswith(\"http\") else base_url + href\n",
        "            })\n",
        "    return articles\n",
        "\n",
        "def scrape_day(base_url_template, base_url, year, month, day, filter_func):\n",
        "    \"\"\"\n",
        "    Scrape headlines for a given day.\n",
        "\n",
        "    Args:\n",
        "        base_url_template (str): Archive URL template with placeholders\n",
        "        base_url (str): Main site URL\n",
        "        year, month, day (int): Date to scrape\n",
        "        filter_func (function): Custom function to filter valid articles\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: Scraped article info\n",
        "    \"\"\"\n",
        "    url = build_archive_url(base_url_template, year, month, day)\n",
        "    publish_date = f\"{year}{month:02d}{day:02d}\"\n",
        "    print(f\"Fetching {publish_date} ...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, verify=False, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        articles = parse_article_links(response.text, base_url, filter_func)\n",
        "        for a in articles:\n",
        "            a[\"publish_date\"] = publish_date\n",
        "        print(f\"{len(articles)} articles found.\")\n",
        "        return articles\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {publish_date}: {e}\")\n",
        "        return []\n",
        "\n",
        "def scrape_year(base_url_template, base_url, year, filter_func):\n",
        "    \"\"\"\n",
        "    Scrape news data for an entire year.\n",
        "\n",
        "    Args:\n",
        "        base_url_template (str): Archive URL template\n",
        "        base_url (str): Base domain\n",
        "        year (int): Year to scrape\n",
        "        filter_func (function): Function to filter article links\n",
        "\n",
        "    Saves:\n",
        "        Excel file of results\n",
        "    \"\"\"\n",
        "    all_articles = []\n",
        "\n",
        "    for month in range(1, 13):\n",
        "        start = date(year, month, 1)\n",
        "        end = (date(year, month + 1, 1) - timedelta(days=1)) if month < 12 else date(year, 12, 31)\n",
        "\n",
        "        for day_offset in range((end - start).days + 1):\n",
        "            current_date = start + timedelta(days=day_offset)\n",
        "            daily_articles = scrape_day(base_url_template, base_url, current_date.year, current_date.month, current_date.day, filter_func)\n",
        "            all_articles.extend(daily_articles)\n",
        "\n",
        "            # Be polite with random delays\n",
        "            time.sleep(random.randint(1, 3))\n",
        "\n",
        "    # Save to Excel\n",
        "    if all_articles:\n",
        "        df = pd.DataFrame(all_articles)\n",
        "        output_file = os.path.join(OUTPUT_PATH, f\"News_Archive_{year}.xlsx\")\n",
        "        df.to_excel(output_file, index=False)\n",
        "        print(f\"Data saved at: {output_file}\")\n",
        "    else:\n",
        "        print(\"No data scraped.\")\n",
        "\n",
        "# Example Usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Customize the following based on your target site\n",
        "    BASE_URL = \"https://example.com\"\n",
        "    ARCHIVE_URL_TEMPLATE = \"https://example.com/archive/{year}/{month}/{day}/start-{starttime}/\"\n",
        "\n",
        "    # Filter function to recognize valid article links (customize for each site)\n",
        "    def article_filter(href):\n",
        "        return \"/news/\" in href and \"/article/\" in href  # Update as needed\n",
        "\n",
        "    # Start the scraper\n",
        "    TARGET_YEAR = 2015\n",
        "    print(f\"Starting scrape for {TARGET_YEAR}...\\n\")\n",
        "    scrape_year(ARCHIVE_URL_TEMPLATE, BASE_URL, TARGET_YEAR, article_filter)\n",
        "    print(\"\\nDone scraping.\")\n"
      ]
    }
  ]
}